---
title: 梯度下降法
date: 2021-11-5 12:30:12
categories:
  - slam
  - 理论学习
tags:
  - slam
---

# 参考网址:

[(60条消息) 高斯牛顿法详解_我只是一只自动小青蛙的博客-CSDN博客](https://blog.csdn.net/qq_42138662/article/details/109289129)

[信赖域狗腿（dogleg）方法_LSEC小陆的博客-CSDN博客](https://blog.csdn.net/lusongno1/article/details/80558684)

[高斯牛顿(Gauss Newton)、列文伯格-马夸尔特(Levenberg-Marquardt)最优化算法与VSLAM_weixin_30463341的博客-CSDN博客](https://blog.csdn.net/weixin_30463341/article/details/94809554?ops_request_misc=%7B%22request%5Fid%22%3A%22163611474316780357221244%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=163611474316780357221244&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-2-94809554.pc_search_mgc_flag&utm_term=列文伯格-马夸尔特算法流程&spm=1018.2226.3001.4187)

[LM算法——列文伯格-马夸尔特算法（最速下降法，牛顿法，高斯牛顿法）（完美解释负梯度方向）_三眼二郎-CSDN博客_lm算法](https://blog.csdn.net/a6333230/article/details/83304098)

# 从上倒下为梯度下降法的前世今生已经未来的演化：

**最速下降法（一阶梯度法）**
**牛顿法（二阶梯度法）**
**高斯牛顿法**
**列文伯格法**
**马夸尔特法**

梯度下降主要用于slam中的非线性优化,实际上就是对一个最小二乘问题的求解,这也是上述几种方法的用途.

# 问题

![在这里插入图片描述](https://img-blog.csdnimg.cn/202010261811320.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMTM4NjYy,size_16,color_FFFFFF,t_70#pic_center)

# 最速下降(一阶梯度法)

**最速下降法**（一阶梯度法）就是保留泰勒展开的一阶项用来近似非线性函数**F ( x )**，即：
$$
F(xk​+Δxk​)≈F(xk​)+J(xk​)TΔxk
$$

$$
Δxk​=−J(xk)
$$

**缺点:**由于仅保留一阶的雅可比矩阵,该方法过于贪心，容易走出锯齿线，反而增加迭代次数。

# 牛顿法和阻尼牛顿法（二阶梯度法）

$$
H(x 
k
​
 )Δx 
k
​
 =−J(x 
k
​
 )
$$

**牛顿法的缺点:**海塞矩阵H计算量太大

**阻尼牛顿法**(可以看成是牛顿法与最速法的结合)

阻尼牛顿法就是在使用牛顿法获得增量方向后，进一步对最优步长进行搜索：

# 高斯牛顿法(仅用于最小二乘)

### 原理(不对优化目标函数进行泰勒展开，我们对优化目标函数中的一部分，即f(x)进行一阶泰勒展开)

**增量方程:**
$$
H(x 
k
​
 )Δx 
k
​
 =g(x 
k
​
 )
$$
**算法流程:**

```shell
1.给定初始值X0
2.对于第k次迭代,求出当前雅可比矩阵J与误差f(x)
3.求解增量方程:H*deltaxk=g
4.若的了他xk足够小,则停止,否则,xk+1 = xk + deltaxk
```

**缺点:**

由于是通过雅各比矩阵做的JH(海瑟矩阵)的近似,因此会遇见奇异矩阵与病态矩阵,可能出现算法不收敛.

# L-M方法，阻尼牛顿法

**列文伯格-马夸尔特方法的思想**
针对高斯牛顿法的不足，L-M方法做了两点改进：

- 在求解增量Δ xk 时，对其设置了信赖区域

- 在求得增量Δ xk对其近似效果进行了量化，并根据量化结果对信赖区域进行调整，

  再从新计算增量Δ x k，直到近似效果量化结果达到阈值。

**增量方程**
$$
(H+λD 
T
 D)Δx 
k
​
 =g(x 
k
​
 )
$$
**近似程度的量化**
$$
ρ= 
(f(xk​+Δx k​)−f(x k​ ))/(J(x k​)TΔxk​)​
$$

- 当ρ接近1时，近似效果好；
- 当ρ太小时，实际减小的值远小于近似函数减小的值，近似效果差，需要缩小近似范围μ
- 当ρ较大时，实际减小的值大于近似函数减小的值，近似效果差，需要增大近似范围μ 

**算法流程:**

![img](https://img-blog.csdn.net/20170712180312420?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemh1YmFvaHVhX2J1cHQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)



